# Load necessary libraries
library(e1071)
library(caret)
library(dplyr)
library(tm)
library(SnowballC)
library(textclean)
library(tictoc)
# Load dataset
reviews <- read.csv("Dataset-SA.csv", stringsAsFactors = FALSE)
# Load dataset
reviews <- read.csv("C:\\Users\\rande\\Downloads\\NaiveBayesAlg\\Dataset-SA.csv", stringsAsFactors = FALSE)
# Convert Sentiment column to factor
reviews$Sentiment <- factor(reviews$Sentiment, levels = c("negative", "neutral", "positive"))
# Text Preprocessing Function
clean_text <- function(text) {
text <- tolower(text)
text <- removePunctuation(text)
text <- removeNumbers(text)
text <- removeWords(text, stopwords("en"))
text <- stripWhitespace(text)
return(text)
}
# Apply Text Cleaning
tic("Text Cleaning")
reviews$cleaned_review <- sapply(reviews$Review, clean_text)
# Load necessary libraries
library(quanteda)
library(quanteda.textmodels)
install.packages("quanteda.textmodels")
# Load necessary libraries
library(quanteda)
library(quanteda.textmodels)
library(caret)
library(tictoc)
library(dplyr)
# Load Dataset
reviews <- read.csv("C:\\Users\\rande\\Downloads\\NaiveBayesAlg\\Dataset-SA.csv", stringsAsFactors = FALSE)
# Convert Sentiment column to factor
reviews$Sentiment <- factor(reviews$Sentiment, levels = c("negative", "neutral", "positive"))
# Start text cleaning timer
tic("Text Cleaning")
# Create a corpus
corpus <- corpus(reviews$Review)
# Tokenize and clean text (removing punctuation, numbers, and stopwords)
tokens <- tokens(corpus,
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_remove(stopwords("en"))
# Create a document-feature matrix (DFM)
dfm_reviews <- dfm(tokens) %>%
dfm_trim(min_termfreq = 10)  # Keep words that appear at least 10 times
# Stop text cleaning timer
toc()
# Split into train and test sets
set.seed(42)
train_index <- sample(seq_len(nrow(dfm_reviews)), size = 0.8 * nrow(dfm_reviews))
dfm_train <- dfm_reviews[train_index, ]
dfm_test  <- dfm_reviews[-train_index, ]
y_train <- reviews$Sentiment[train_index]
y_test  <- reviews$Sentiment[-train_index]
# Convert labels to factors for caret
y_train <- factor(y_train, levels = levels(reviews$Sentiment))
y_test  <- factor(y_test, levels = levels(reviews$Sentiment))
# Start training timer
tic("Training Time")
# Train Naïve Bayes Model
nb_model <- textmodel_nb(dfm_train, y_train)
# Stop training timer
train_time <- toc()
# Start testing timer
tic("Testing Time")
# Make Predictions
y_pred <- predict(nb_model, newdata = dfm_test)
# Stop testing timer
test_time <- toc()
# Compute Confusion Matrix
conf_matrix_nb <- confusionMatrix(y_pred, y_test)
# Extract Metrics
accuracy_nb <- conf_matrix_nb$overall["Accuracy"]
weighted_f1_nb <- mean(conf_matrix_nb$byClass[, "F1"], na.rm = TRUE)
# Print Results
print(conf_matrix_nb)
print(paste("Accuracy:", round(accuracy_nb * 100, 2), "%"))
print(paste("Weighted F1 Score:", round(weighted_f1_nb * 100, 2), "%"))
print(paste("Training Time:", round(train_time$toc - train_time$tic, 2), "seconds"))
print(paste("Testing Time:", round(test_time$toc - test_time$tic, 2), "seconds"))
# Load necessary libraries
library(quanteda)
library(quanteda.textmodels)
library(caret)
library(tictoc)
library(dplyr)
# Load Dataset
reviews <- read.csv("Dataset-SA.csv", stringsAsFactors = FALSE)
# Convert Sentiment column to factor
reviews$Sentiment <- factor(reviews$Sentiment, levels = c("negative", "neutral", "positive"))
# Start text cleaning timer
tic("Text Cleaning")
# Create a corpus
corpus <- corpus(reviews$Review)
# Tokenization with improved preprocessing
tokens <- tokens(corpus,
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("en")) %>%
tokens_wordstem()  # Stemming (e.g., "running" → "run")
# Create Document-Feature Matrix (DFM) with N-grams
dfm_reviews <- dfm(tokens, ngrams = 1:2) %>%  # Use unigrams & bigrams
dfm_trim(min_termfreq = 5, max_docfreq = 0.8)  # Remove rare & overly common words
# Apply TF-IDF Weighting
dfm_reviews <- dfm_weight(dfm_reviews, scheme = "tfidf")
# Stop text cleaning timer
toc()
# Split into train and test sets
set.seed(42)
train_index <- sample(seq_len(nrow(dfm_reviews)), size = 0.8 * nrow(dfm_reviews))
dfm_train <- dfm_reviews[train_index, ]
dfm_test  <- dfm_reviews[-train_index, ]
y_train <- reviews$Sentiment[train_index]
y_test  <- reviews$Sentiment[-train_index]
# Convert labels to factors for caret
y_train <- factor(y_train, levels = levels(reviews$Sentiment))
y_test  <- factor(y_test, levels = levels(reviews$Sentiment))
# Start training timer
tic("Training Time")
# Train Complement Naïve Bayes (Better for imbalanced data)
nb_model <- textmodel_nb(dfm_train, y_train, smooth = 1, prior = "uniform")
# Load necessary libraries
library(quanteda)
library(quanteda.textmodels)
library(caret)
library(tictoc)
library(dplyr)
# Load Dataset
reviews <- read.csv("C:\\Users\\rande\\Downloads\\NaiveBayesAlg\\Dataset-SA.csv", stringsAsFactors = FALSE)
# Convert Sentiment column to factor
reviews$Sentiment <- factor(reviews$Sentiment, levels = c("negative", "neutral", "positive"))
# Start text cleaning timer
tic("Text Cleaning")
# Create a corpus
corpus <- corpus(reviews$Review)
# Tokenization with improved preprocessing
tokens <- tokens(corpus,
remove_punct = TRUE,
remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("en")) %>%
tokens_wordstem()  # Stemming (e.g., "running" → "run")
# Create Document-Feature Matrix (DFM) with N-grams
dfm_reviews <- dfm(tokens, ngrams = 1:2) %>%  # Use unigrams & bigrams
dfm_trim(min_termfreq = 5, max_docfreq = 0.8)  # Remove rare & overly common words
# Apply TF-IDF Weighting
dfm_reviews <- dfm_weight(dfm_reviews, scheme = "tfidf")
# Stop text cleaning timer
toc()
# Split into train and test sets
set.seed(42)
train_index <- sample(seq_len(nrow(dfm_reviews)), size = 0.8 * nrow(dfm_reviews))
dfm_train <- dfm_reviews[train_index, ]
dfm_test  <- dfm_reviews[-train_index, ]
y_train <- reviews$Sentiment[train_index]
y_test  <- reviews$Sentiment[-train_index]
# Convert labels to factors for caret
y_train <- factor(y_train, levels = levels(reviews$Sentiment))
y_test  <- factor(y_test, levels = levels(reviews$Sentiment))
# Start training timer
tic("Training Time")
# Train Complement Naïve Bayes (Better for imbalanced data)
nb_model <- textmodel_nb(dfm_train, y_train, smooth = 1, prior = "uniform")
print(dim(dfm_reviews))  # Should show non-zero values
print(topfeatures(dfm_reviews, 20))  # Should list top 20 words
dfm_trim(min_termfreq = 5, max_docfreq = 0.8)
dfm_reviews <- dfm_trim(dfm_reviews, min_termfreq = 2, max_docfreq = 0.9)
print(head(tokens))
dfm_reviews <- dfm(tokens, ngrams = 1:2)
dfm_reviews <- dfm_weight(dfm_reviews, scheme = "tfidf")
dfm_reviews <- dfm_trim(dfm_reviews, min_termfreq = 2, max_docfreq = 0.9)
library(quanteda)
# Tokenization
tokens <- tokens(reviews$Review, remove_punct = TRUE, remove_numbers = TRUE)
tokens <- tokens_remove(tokens, stopwords("en"))  # Remove stopwords
tokens <- tokens_wordstem(tokens)  # Apply stemming
# Create Document-Feature Matrix (DFM)
dfm_reviews <- dfm(tokens, ngrams = 1:2)  # Create with unigrams & bigrams
dfm_reviews <- dfm_trim(dfm_reviews, min_termfreq = 2, max_docfreq = 0.9)  # Keep more terms
# Check if DFM is non-empty
print(dim(dfm_reviews))  # Should not return (0,0)
print(topfeatures(dfm_reviews, 20))  # Should display frequent words
print(dim(dfm_reviews))  # Should show rows (docs) and columns (terms)
print(summary(dfm_reviews))  # Provides an overview
print(ndoc(dfm_reviews))  # Number of documents (should match review count)
print(nfeat(dfm_reviews))  # Number of features (should be > 0)
topfeatures(dfm_reviews, 20)  # Shows top words with counts
as.matrix(dfm_reviews)[1:5, 1:5]  # Displays a small portion
